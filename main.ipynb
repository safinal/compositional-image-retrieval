{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nafisi/temp/tmp/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, model_name='ViT-B-32', pretrained='laion2b_s34b_b79k') -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = open_clip.get_tokenizer(model_name)\n",
    "        self.feature_extractor, _, self.processor = open_clip.create_model_and_transforms(\n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        \n",
    "        # Get CLIP embedding dimension\n",
    "        self.embed_dim = self.feature_extractor.visual.output_dim\n",
    "        \n",
    "        # Additional projection layers\n",
    "        self.query_projection = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.embed_dim * 2, self.embed_dim),\n",
    "            torch.nn.LayerNorm(self.embed_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        )\n",
    "        \n",
    "        self.database_projection = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.embed_dim, self.embed_dim),\n",
    "            torch.nn.LayerNorm(self.embed_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        )\n",
    "        \n",
    "        self.set_param_trainable_mode(module=self.feature_extractor, status=False)\n",
    "\n",
    "\n",
    "    def set_param_trainable_mode(self, module, status):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = status\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path, weights_only=True))\n",
    "\n",
    "    def forward(self, query_image, query_text):\n",
    "        # Get base embeddings from CLIP\n",
    "        image_features = self.feature_extractor.encode_image(query_image)\n",
    "        text_features = self.feature_extractor.encode_text(query_text)\n",
    "        \n",
    "        # Concatenate image and text features\n",
    "        combined_features = torch.cat([image_features, text_features], dim=1)\n",
    "        \n",
    "        # Project through learnable layers\n",
    "        query_embedding = self.query_projection(combined_features)\n",
    "        \n",
    "        return query_embedding\n",
    "    \n",
    "    def encode_database_image(self, image):\n",
    "        image_features = self.feature_extractor.encode_image(image)\n",
    "        database_embedding = self.database_projection(image_features)\n",
    "        return database_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoNCELoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, query_embeds, database_embeds):\n",
    "        \"\"\"\n",
    "        InfoNCE loss implementation\n",
    "        \n",
    "        Args:\n",
    "            query_embeds: Query embeddings [batch_size, embed_dim]\n",
    "            database_embeds: Database embeddings [batch_size, embed_dim]\n",
    "            \n",
    "        Returns:\n",
    "            loss: InfoNCE loss value\n",
    "        \"\"\"\n",
    "        # Normalize embeddings\n",
    "        query_embeds = torch.nn.functional.normalize(query_embeds, dim=1)\n",
    "        database_embeds = torch.nn.functional.normalize(database_embeds, dim=1)\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = torch.matmul(query_embeds, database_embeds.T) / self.temperature\n",
    "        \n",
    "        # Labels are the diagonal elements (positive pairs)\n",
    "        labels = torch.arange(len(query_embeds)).to(query_embeds.device)\n",
    "        \n",
    "        # Calculate loss in both directions (query->database and database->query)\n",
    "        loss_q2d = self.criterion(similarity_matrix, labels)\n",
    "        loss_d2q = self.criterion(similarity_matrix.T, labels)\n",
    "        \n",
    "        # Total loss is the average of both directions\n",
    "        return (loss_q2d + loss_d2q) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.95\n",
    "IMAGE_ROOT_DIR = os.path.join(os.getcwd(), 'dataset', 'images')\n",
    "ANNOTATIONS_FILE_PATH = os.path.join(os.getcwd(), 'dataset', 'data.csv')\n",
    "TEST_ROOT_DIR = os.path.join(os.getcwd(), 'sample_evaluation', 'images')\n",
    "TEST_ANNOTATIONS_FILE_PATH = os.path.join(os.getcwd(), 'sample_evaluation', 'data.csv')\n",
    "BATCH_SIZE = 80\n",
    "NUM_WORKERS = 128\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "MODEL_NAME = 'ViT-B-32'\n",
    "PRETRAINED_WEIGHTS = 'laion2b_s34b_b79k'\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOSS_TEMPERATURE = 0.07\n",
    "SCHEDULER_T_0 = 5\n",
    "SCHEDULER_T_MULT = 2\n",
    "\n",
    "\n",
    "model = Model(model_name=MODEL_NAME, pretrained=PRETRAINED_WEIGHTS).to(DEVICE)\n",
    "criterion = InfoNCELoss(temperature=LOSS_TEMPERATURE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=SCHEDULER_T_0, T_mult=SCHEDULER_T_MULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir_path: str, annotations_file_path: str, split: str, transform=None, tokenizer=None) -> None:\n",
    "        self.img_dir_path = img_dir_path\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.split = split\n",
    "        self.annotations = self.split_data(\n",
    "            # self.data_health_check(\n",
    "                self.convert_image_names_to_path(\n",
    "                    pd.read_csv(annotations_file_path)\n",
    "                )\n",
    "            # )\n",
    "        )\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        query_img_path = self.annotations.iloc[idx]['query_image']\n",
    "        query_text = self.annotations.iloc[idx]['query_text']\n",
    "        target_img_path = self.annotations.iloc[idx]['target_image']\n",
    "        query_img = Image.open(query_img_path).convert('RGB')\n",
    "        target_img = Image.open(target_img_path).convert('RGB')\n",
    "        # query_img = torchvision.io.read_image(path=query_img_path, mode=torchvision.io.image.ImageReadMode.RGB)\n",
    "        # target_img = torchvision.io.read_image(path=target_img_path, mode=torchvision.io.image.ImageReadMode.RGB)\n",
    "        if self.transform:\n",
    "            query_img = self.transform(query_img)\n",
    "            target_img = self.transform(target_img)\n",
    "        if self.tokenizer:\n",
    "            query_text = self.tokenizer(query_text).squeeze(0)\n",
    "        return query_img, query_text, target_img\n",
    "    \n",
    "    def split_data(self, annotations):\n",
    "        shuffled_df = annotations.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        if self.split == \"test\":\n",
    "            return shuffled_df # sample test set\n",
    "        if self.split == \"train\":\n",
    "            return shuffled_df.iloc[:int(SPLIT_RATIO * len(shuffled_df))] # train set\n",
    "        if self.split == \"validation\":\n",
    "            return shuffled_df.iloc[int(SPLIT_RATIO * len(shuffled_df)):] # validation set\n",
    "        raise Exception(\"split is not valid\")\n",
    "\n",
    "    def load_queries(self):\n",
    "        return self.annotations.drop(columns=[\"target_image\"])\n",
    "    \n",
    "    def load_database(self):\n",
    "        return self.annotations[[\"target_image\"]]\n",
    "    \n",
    "    def convert_image_names_to_path(self, df):\n",
    "        df[\"query_image\"] = self.img_dir_path + \"/\" + df[\"query_image\"]\n",
    "        df[\"target_image\"] = self.img_dir_path + \"/\" + df[\"target_image\"]\n",
    "        return df\n",
    "    \n",
    "    # def data_health_check(self, annotations):\n",
    "    #     img_files = os.listdir(self.img_dir_path)\n",
    "    #     broken_files = [img for img in img_files if self.is_truncated(os.path.join(self.img_dir_path, img))]\n",
    "    #     annotations = annotations[\n",
    "    #         ~annotations['target_image'].isin(broken_files) &\n",
    "    #         ~annotations['query_image'].isin(broken_files)\n",
    "    #     ]\n",
    "    #     return annotations\n",
    "    \n",
    "    # def is_truncated(self, image_path):\n",
    "    #     try:\n",
    "    #         with Image.open(image_path) as img:\n",
    "    #             img.verify()\n",
    "    #         return False\n",
    "    #     except (IOError, SyntaxError, Image.DecompressionBombError) as e:\n",
    "    #         return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueTargetImageBatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        Initializes the sampler.\n",
    "\n",
    "        Args:\n",
    "            dataset (RetrievalDataset): The dataset to sample from.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "            shuffle (bool): Whether to shuffle the data every epoch.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Create a mapping from target_image to list of indices\n",
    "        self.target_to_indices = defaultdict(list)\n",
    "        for idx in range(len(self.dataset)):\n",
    "            target_image = self.dataset.annotations.iloc[idx]['target_image']\n",
    "            self.target_to_indices[target_image].append(idx)\n",
    "        \n",
    "        # List of unique target_images\n",
    "        self.unique_target_images = list(self.target_to_indices.keys())\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.unique_target_images)\n",
    "            for indices in self.target_to_indices.values():\n",
    "                random.shuffle(indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields lists of indices where each list represents a batch with unique target_images.\n",
    "        \"\"\"\n",
    "        # Create a copy of indices per target_image to preserve original order\n",
    "        queues = [indices.copy() for indices in self.target_to_indices.values()]\n",
    "        \n",
    "        if self.shuffle:\n",
    "            random.shuffle(queues)\n",
    "        \n",
    "        batch = []\n",
    "        while any(queues):\n",
    "            for queue in queues:\n",
    "                if queue:\n",
    "                    batch.append(queue.pop())\n",
    "                    if len(batch) == self.batch_size:\n",
    "                        yield batch\n",
    "                        batch = []\n",
    "            # Optional: Shuffle queues after each full pass to ensure randomness\n",
    "            if self.shuffle:\n",
    "                random.shuffle(queues)\n",
    "        \n",
    "        if batch:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of batches per epoch.\n",
    "        \"\"\"\n",
    "        total = len(self.dataset)\n",
    "        return (total + self.batch_size - 1) // self.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = v2.Compose([\n",
    "    v2.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = v2.Compose([\n",
    "    v2.Resize(256),\n",
    "    v2.CenterCrop(224),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = RetrievalDataset(\n",
    "    img_dir_path=IMAGE_ROOT_DIR,\n",
    "    annotations_file_path=ANNOTATIONS_FILE_PATH,\n",
    "    split='train',\n",
    "    transform=model.processor if hasattr(model, 'processor') else test_transform,\n",
    "    tokenizer=model.tokenizer\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    # batch_size=BATCH_SIZE, \n",
    "    # shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    batch_sampler=UniqueTargetImageBatchSampler(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    ")\n",
    "\n",
    "val_dataset = RetrievalDataset(\n",
    "    img_dir_path=IMAGE_ROOT_DIR,\n",
    "    annotations_file_path=ANNOTATIONS_FILE_PATH,\n",
    "    split='validation',\n",
    "    transform=model.processor if hasattr(model, 'processor') else test_transform,\n",
    "    tokenizer=model.tokenizer\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "test_dataset = RetrievalDataset(\n",
    "    img_dir_path=TEST_ROOT_DIR,\n",
    "    annotations_file_path=TEST_ANNOTATIONS_FILE_PATH,\n",
    "    split='test',\n",
    "    transform=model.processor if hasattr(model, 'processor') else test_transform,\n",
    "    tokenizer=model.tokenizer\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_actions(text):\n",
    "    # Load English language model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # List of common action verbs and their normalized forms\n",
    "    action_verbs = {\n",
    "        'add': 'add',\n",
    "        'insert': 'add',\n",
    "        'introduce': 'add',\n",
    "        'bring': 'add',\n",
    "        'place': 'add',\n",
    "        'include': 'add',\n",
    "        'remove': 'remove',\n",
    "        'eliminate': 'remove',\n",
    "        'discard': 'remove',\n",
    "        'take': 'remove',\n",
    "        'get rid': 'remove'\n",
    "    }\n",
    "    \n",
    "    # Process the text\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    # Initialize result structure\n",
    "    result = [\n",
    "        {\"verb\": [], \"nouns\": []},\n",
    "        {\"verb\": [], \"nouns\": []}\n",
    "    ]\n",
    "    \n",
    "    # Split text into two parts if possible\n",
    "    parts = re.split(r'\\s+(?:and|,|then|\\.|\\s+)+\\s*', text.lower())\n",
    "    \n",
    "    current_action = 0\n",
    "    current_verb = None\n",
    "    \n",
    "    for token in doc:\n",
    "        # Check for verbs\n",
    "        lemma = token.lemma_.lower()\n",
    "        if any(verb in lemma for verb in action_verbs.keys()):\n",
    "            # Handle multi-word verbs\n",
    "            verb_phrase = lemma\n",
    "            if token.i + 1 < len(doc) and doc[token.i + 1].text in ['in', 'away', 'out']:\n",
    "                verb_phrase += ' ' + doc[token.i + 1].text\n",
    "            \n",
    "            normalized_verb = None\n",
    "            for verb, norm in action_verbs.items():\n",
    "                if verb in verb_phrase:\n",
    "                    normalized_verb = norm\n",
    "                    break\n",
    "            \n",
    "            if normalized_verb:\n",
    "                if current_verb != normalized_verb:\n",
    "                    current_verb = normalized_verb\n",
    "                    if current_action < 2:\n",
    "                        result[current_action][\"verb\"] = [normalized_verb]\n",
    "                        current_action += 1\n",
    "        \n",
    "        # Collect nouns\n",
    "        elif token.pos_ == \"NOUN\":\n",
    "            if current_action > 0 and len(result[current_action-1][\"verb\"]) > 0:\n",
    "                if token.text not in result[current_action-1][\"nouns\"]:\n",
    "                    result[current_action-1][\"nouns\"].append(token.text)\n",
    "    \n",
    "    # Clean up empty actions and ensure proper structure\n",
    "    for action in result:\n",
    "        if not action[\"verb\"]:\n",
    "            action[\"verb\"] = [\"add\" if not result[0][\"verb\"] else \"remove\"]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_queries(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Process query pairs and generate embeddings.\n",
    "\n",
    "    Args:\n",
    "    df (pd. DataFrame ): DataFrame with columns:\n",
    "    - query_image: str, paths to query images\n",
    "    - query_text: str, text descriptions\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Embeddings array (num_queries, embedding_dim)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(df), BATCH_SIZE)):\n",
    "        query_imgs = torch.stack([model.processor(Image.open(query_image_path)) for query_image_path in df['query_image'][i:i+BATCH_SIZE]]).to(DEVICE)\n",
    "        samples = []\n",
    "        for sample in df['query_text'][i:i+BATCH_SIZE].map(lambda text: parse_actions(text)):\n",
    "            with torch.no_grad():\n",
    "                samples.append({sample[0]['verb'][0]: model.feature_extractor.encode_text(model.tokenizer(list(map(lambda x: \"a photo of \" + x, sample[0]['nouns']))).to(DEVICE)), sample[1]['verb'][0]: model.feature_extractor.encode_text(model.tokenizer(list(map(lambda x: \"a photo of \" + x, sample[1]['nouns']))).to(DEVICE))})\n",
    "        # query_texts = model.tokenizer(df['query_text'][i:i+BATCH_SIZE]).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            # query_embedding = model(query_imgs, query_texts)\n",
    "            query_embedding = model.feature_extractor.encode_image(query_imgs)\n",
    "        assert len(samples) == len(query_embedding)\n",
    "        for j in range(len(samples)):\n",
    "            for val in samples[j]['add']:\n",
    "                query_embedding[j] += val\n",
    "            for val in samples[j]['remove']:\n",
    "                query_embedding[j] -= val\n",
    "        query_embedding = torch.nn.functional.normalize(query_embedding, dim=1, p=2)\n",
    "        all_embeddings.append(query_embedding.detach().cpu().numpy())\n",
    "    return np.concatenate(all_embeddings)\n",
    "\n",
    "\n",
    "def encode_database(df: pd.DataFrame) -> np.ndarray :\n",
    "    \"\"\"\n",
    "    Process database images and generate embeddings.\n",
    "\n",
    "    Args:\n",
    "    df (pd. DataFrame ): DataFrame with column:\n",
    "    - target_image: str, paths to database images\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Embeddings array (num_images, embedding_dim)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(df), BATCH_SIZE)):\n",
    "        target_imgs = torch.stack([model.processor(Image.open(target_image_path)) for target_image_path in df['target_image'][i:i+BATCH_SIZE]]).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            # target_imgs_embedding = model.encode_database_image(target_imgs)\n",
    "            target_imgs_embedding = model.feature_extractor.encode_image(target_imgs)\n",
    "        target_imgs_embedding = torch.nn.functional.normalize(target_imgs_embedding, dim=1, p=2)\n",
    "        all_embeddings.append(target_imgs_embedding.detach().cpu().numpy())\n",
    "    return np.concatenate(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions: np.ndarray, ground_truth: np.ndarray) -> float:\n",
    "    assert predictions.shape == ground_truth.shape, \"Predictions and ground truth must have the same shape.\"\n",
    "    \n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = (predictions == ground_truth).sum()\n",
    "    total_predictions = len(predictions)\n",
    "    \n",
    "    # Calculate accuracy as a percentage\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "def evaluate(dataset):\n",
    "    query_embeddings = encode_queries(dataset.load_queries())\n",
    "    database_embeddings = encode_database(dataset.load_database())\n",
    "    similarities = cosine_similarity(query_embeddings, database_embeddings)\n",
    "    predictions = np.argmax(similarities, axis=1)\n",
    "    ground_truth = np.arange(len(database_embeddings))\n",
    "    accuracy = calculate_accuracy(predictions, ground_truth)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Accuracy: {100*evaluate(val_dataset)}\")\n",
    "print(f\"Test Accuracy: {100*evaluate(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with tqdm(train_loader, desc=\"Training\") as pbar:\n",
    "        for batch_idx, (query_imgs, query_texts, target_imgs) in enumerate(pbar):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Move data to device\n",
    "            query_imgs = query_imgs.to(device)\n",
    "            target_imgs = target_imgs.to(device)\n",
    "            query_texts = query_texts.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            query_embeds = model(query_imgs, query_texts)\n",
    "            database_embeds = model.encode_database_image(target_imgs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(query_embeds, database_embeds)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'avg_loss': f\"{total_loss / (batch_idx + 1):.4f}\"\n",
    "            })\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for query_imgs, query_texts, target_imgs in tqdm(loader, desc=\"Validation\"):\n",
    "            # Move data to device\n",
    "            query_imgs = query_imgs.to(device)\n",
    "            target_imgs = target_imgs.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            query_embeds = model(query_imgs, query_texts)\n",
    "            database_embeds = model.encode_database_image(target_imgs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(query_embeds, database_embeds)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            similarity = torch.matmul(query_embeds, database_embeds.T)\n",
    "            predictions = similarity.argmax(dim=1)\n",
    "            labels = torch.arange(len(predictions)).to(device)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(predictions)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "model.set_param_trainable_mode(model.feature_extractor, status=True)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    print(f\"Test Accuracy: {100*evaluate(test_dataset)}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:49<00:00, 109.09s/it]\n",
      "100%|██████████| 1/1 [01:47<00:00, 107.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 67.08860759493672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.18s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 55.00000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation Accuracy: {100*evaluate(val_dataset)}\")\n",
    "print(f\"Test Accuracy: {100*evaluate(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/home/nafisi/temp/rayan-phase2-q1/weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"/home/nafisi/temp/rayan-phase2-q1/weights.pth\",\n",
    "    path_in_repo=\"weights.pth\",\n",
    "    repo_id=\"safinal/rayan-phase2-q1\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
