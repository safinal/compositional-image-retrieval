model:
  pretrained: "distilbert-base-uncased"

training:
  output_dir: "./trained_distil_bert_base_results"
  num_train_epochs: 20
  per_device_train_batch_size: 1024
  per_device_eval_batch_size: 1024
  warmup_steps: 500
  weight_decay: 0.01
  logging_dir: "./trained_distil_bert_base_logs"
  logging_steps: 10
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  save_total_limit: 2
  save_dir: "./trained_distil_bert_base"
  
data:
  objects_path: "./token_classification_data/objects.txt"
  prompt_templates_path: "./token_classification_data/prompt_templates.json"
  num_data_per_prompt_template: 15
